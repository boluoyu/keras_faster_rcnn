{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-faster-rcnn\n",
    "\n",
    "--------\n",
    "This project aim the implement faster rcnn in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib.framework.python.ops import arg_scope\n",
    "from tensorflow.contrib.layers.python.layers import layers as layers_lib\n",
    "from tensorflow.contrib.layers.python.layers import regularizers\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import variable_scope\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from math import floor,exp\n",
    "import pprint\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the config variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "anchor_box_scales = [128, 256, 512]\n",
    "anchor_box_ratio = [[1,1],[1,2],[2,1]]\n",
    "\n",
    "\n",
    "TEST_FULL_IMG = np.array([mpimg.imread(\"./test1.jpg\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 720, 1280, 3)\n",
      "180.0 320.0\n"
     ]
    }
   ],
   "source": [
    "print(TEST_FULL_IMG.shape)\n",
    "print(TEST_FULL_IMG.shape[1]/4, TEST_FULL_IMG.shape[2]/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data generator \n",
    "\n",
    "We use VOC 2012 Datasets. I will define funstions to prepare the datasets.\n",
    "\n",
    "--------\n",
    "Steps:\n",
    "1. Read teh annotations files.\n",
    "2. Create the ground  turth data sets if ground truth does not exist for that image.\n",
    "3. Define a genertor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "import os\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define funciton for get the data from the annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataSets = ['VOC2012']\n",
    "\n",
    "\n",
    "def get_vox_data(input_path):\n",
    "    all_imgs = []\n",
    "\n",
    "    classes_count = {}\n",
    "\n",
    "    class_mappingNameToId = {}\n",
    "    class_mappingIdToName = {}\n",
    "\n",
    "    visualise = False\n",
    "\n",
    "    data_paths = [os.path.join(input_path, s) for s in dataSets]\n",
    "\n",
    "    print ('Parsing annotation files')\n",
    "\n",
    "    for data_path in data_paths:\n",
    "\n",
    "        annot_path = os.path.join(data_path, 'Annotations')\n",
    "        imgs_path = os.path.join(data_path, 'JPEGImages')\n",
    "        imgsets_path_trainval = os.path.join(data_path, 'ImageSets',\n",
    "                'Main', 'trainval.txt')\n",
    "\n",
    "# ........imgsets_path_test = os.path.join(data_path, 'ImageSets','Main','test.txt')\n",
    "\n",
    "        trainval_files = []\n",
    "        test_files = []\n",
    "        try:\n",
    "            with open(imgsets_path_trainval) as f:\n",
    "                for line in f:\n",
    "                    trainval_files.append(line.strip() + '.jpg')\n",
    "        except Exception as e:\n",
    "\n",
    "# ............with open(imgsets_path_test) as f:\n",
    "# ................for line in f:\n",
    "# ....................test_files.append(line.strip() + '.jpg')\n",
    "\n",
    "            print (e)\n",
    "\n",
    "        annots = [os.path.join(annot_path, s) for s in\n",
    "                  os.listdir(annot_path)]\n",
    "        idx = 0\n",
    "        for annot in annots:\n",
    "            try:\n",
    "                idx += 1\n",
    "\n",
    "                et = ET.parse(annot)\n",
    "                element = et.getroot()\n",
    "\n",
    "                element_objs = element.findall('object')\n",
    "                element_filename = element.find('filename').text\n",
    "                element_width = int(element.find('size').find('width'\n",
    "                                    ).text)\n",
    "                element_height = int(element.find('size').find('height'\n",
    "                        ).text)\n",
    "\n",
    "                if len(element_objs) > 0:\n",
    "                    annotation_data = {\n",
    "                        'filepath': os.path.join(imgs_path,\n",
    "                                element_filename),\n",
    "                        'width': element_width,\n",
    "                        'height': element_height,\n",
    "                        'bboxes': [],\n",
    "                        }\n",
    "\n",
    "                    if element_filename in trainval_files:\n",
    "                        annotation_data['imageset'] = 'trainval'\n",
    "                    elif element_filename in test_files:\n",
    "                        annotation_data['imageset'] = 'test'\n",
    "                    else:\n",
    "                        annotation_data['imageset'] = 'trainval'\n",
    "\n",
    "                for element_obj in element_objs:\n",
    "                    class_name = element_obj.find('name').text\n",
    "                    if class_name not in classes_count:\n",
    "                        classes_count[class_name] = 1\n",
    "                    else:\n",
    "                        classes_count[class_name] += 1\n",
    "\n",
    "                    if class_name not in class_mappingNameToId:\n",
    "                        class_mappingNameToId[class_name] = \\\n",
    "                            len(class_mappingNameToId)\n",
    "                        class_mappingIdToName[len(class_mappingNameToId)\n",
    "                                - 1] = class_name\n",
    "\n",
    "                    obj_bbox = element_obj.find('bndbox')\n",
    "                    x1 = int(round(float(obj_bbox.find('xmin').text)))\n",
    "                    y1 = int(round(float(obj_bbox.find('ymin').text)))\n",
    "                    x2 = int(round(float(obj_bbox.find('xmax').text)))\n",
    "                    y2 = int(round(float(obj_bbox.find('ymax').text)))\n",
    "                    w = x2 - x1\n",
    "                    h = y2 - y1\n",
    "                    x = int(round(x1 + w / 2))\n",
    "                    y = int(round(y1 + h / 2))\n",
    "                    difficulty = int(element_obj.find('difficult'\n",
    "                            ).text) == 1\n",
    "                    annotation_data['bboxes'].append({\n",
    "                        'class': class_name,\n",
    "                        'x1': x1,\n",
    "                        'x2': x2,\n",
    "                        'y1': y1,\n",
    "                        'y2': y2,\n",
    "                        'difficult': difficulty,\n",
    "                        'x': x,\n",
    "                        'y': y,\n",
    "                        'w': w,\n",
    "                        'h': h,\n",
    "                        })\n",
    "                all_imgs.append(annotation_data)\n",
    "\n",
    "                if visualise:\n",
    "                    img = cv2.imread(annotation_data['filepath'])\n",
    "                    for bbox in annotation_data['bboxes']:\n",
    "                        cv2.rectangle(img, (bbox['x1'], bbox['y1']),\n",
    "                                (bbox['x2'], bbox['y2']), (0, 0, 255))\n",
    "                    cv2.imshow('img', img)\n",
    "                    cv2.waitKey(0)\n",
    "            except Exception as  e:\n",
    "\n",
    "                print (e)\n",
    "                continue\n",
    "\n",
    "        # make if no bg in the className make bg class\n",
    "\n",
    "        if 'bg' not in classes_count:\n",
    "            print ('bg not in class')\n",
    "            classes_count['bg'] = 0\n",
    "            class_mappingNameToId['bg'] = len(class_mappingNameToId)\n",
    "            class_mappingIdToName[len(class_mappingNameToId)] = 'bg'\n",
    "        else:\n",
    "            print ('bg in class')\n",
    "\n",
    "        # if there are a bg class make it last index\n",
    "\n",
    "            tempId = class_mappingNameToId['bg']\n",
    "            class_mappingNameToId['bg'] = len(class_mappingNameToId) - 1\n",
    "            lastName = class_mappingIdToName[len(class_mappingNameToId)\n",
    "                    - 1]\n",
    "            class_mappingNameToId[lastName] = tempId\n",
    "            class_mappingIdToName[tempId] = lastName\n",
    "            class_mappingIdToName[len(class_mappingNameToId)] = 'bg'\n",
    "            \n",
    "    return (all_imgs, classes_count, class_mappingNameToId, class_mappingIdToName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the function for generating ground truth \n",
    "--------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_ground_truth():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotation files\n",
      "bg not in class\n"
     ]
    }
   ],
   "source": [
    "all_imgs, classes_count, class_mappingNameToId, class_mappingIdToName = get_vox_data(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network \n",
    "Vgg 16 used in this project, it use the pretrain network from imageNet. The network defined until conv5.\n",
    "\n",
    "The vgg16 function accept iamge input and otput the nets tensor and the enpoints.\n",
    "\n",
    "We use conv5 as our feature map layer, which will port to rpn and rcnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the vgg16 layers\n",
    "def vgg_16(inputs,  scope='vgg_16'):\n",
    "    with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n",
    "        end_points_collection = sc.name + '_end_points'\n",
    "\n",
    "        # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d], outputs_collections=end_points_collection):\n",
    "            net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "#             net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "#             net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "\n",
    "            # Convert end_points_collection into a end_point dict.\n",
    "            end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "        \n",
    "    return net, end_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RPN netowrk\n",
    "\n",
    "we will define the RPN network. \n",
    "\n",
    "It have few steps. \n",
    "1. Define the conv layers of rpn.\n",
    "2. Mapping rpn to bbox\n",
    "3. Poposal layer, filter the bbox that needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rpn(net, num_anchors=9, scope=\"rpn\"):\n",
    "    with tf.variable_scope(scope, 'rpn', [net]) as sc:\n",
    "        end_points_collection = sc.name + '_end_points'\n",
    "        \n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d], \n",
    "                            outputs_collections=end_points_collection, \n",
    "                            activation_fn=tf.nn.relu,\n",
    "                            weights_initializer=tf.truncated_normal_initializer(0.0, 0.01)):\n",
    "            \n",
    "            net = slim.conv2d(net, 512, [3, 1], scope='rpn_conv_3x3', padding='SAME')\n",
    "            \n",
    "            rpn_class = slim.conv2d(net, num_anchors, [1, 1], scope='rpn_class')\n",
    "            \n",
    "            \n",
    "            rpn_regr = slim.conv2d(net, num_anchors*4, [1, 1], scope='rpn_regr')   \n",
    "\n",
    "            # Convert end_points_collection into a end_point dict.\n",
    "            end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "        \n",
    "    return rpn_class, rpn_regr, end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define mapping from network result to bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO use transpose to speed up https://www.tensorflow.org/api_docs/python/tf/transpose\n",
    "def mapAnchorToBoxs(rpn_class, rpn_regr, feature_map_ratio, ratio=[(1,1),(2,1),(1,2)], pixel=[64,128,512]):\n",
    "    \n",
    "    class_res  = [];\n",
    "    regr_res = [];\n",
    "    \n",
    "    \n",
    "    for row in range(rpn_class.shape[0]):\n",
    "        for col in range(rpn_class.shape[1]):\n",
    "            i = 0\n",
    "            for r in ratio:\n",
    "                j = 0\n",
    "                for p in pixel:\n",
    "                    anchor_width = r[0]*p / feature_map_ratio\n",
    "                    anchor_height = r[1]*p / feature_map_ratio\n",
    "                    \n",
    "                    p_anchor = rpn_regr[row][col][i+j]\n",
    "                    \n",
    "                    cx = row + p_anchor[0]*anchor_width\n",
    "                    cy = col + p_anchor[1]*anchor_height\n",
    "                    pred_w = exp(p_anchor[2]) * anchor_width\n",
    "                    pred_h = exp(p_anchor[3]) * anchor_height\n",
    "                    \n",
    "                    x1 = int(cx - 0.5 * pred_w)\n",
    "                    y1 = int(cy - 0.5 * pred_h)\n",
    "                    x2 = int(cx + 0.5 * pred_w)\n",
    "                    y2 = int(cy + 0.5 * pred_h)\n",
    "                    \n",
    "                    regr_res.append([x1,y1,x2,y2])\n",
    "                    class_res.append(rpn_class[row][col][i+j])\n",
    "#                     print(\"rpn_class[row][col][i+j]\",rpn_class[row][col][i+j])\n",
    "                    \n",
    "                    \n",
    "                    j+=1\n",
    "                i+=1\n",
    "    \n",
    "    return np.array(class_res), np.array(regr_res)\n",
    "\n",
    "\n",
    "# def bbox_transform_inv(boxes, deltas):\n",
    "#     if boxes.shape[0] == 0:\n",
    "#         return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n",
    "\n",
    "#     boxes = boxes.astype(deltas.dtype, copy=False)\n",
    "\n",
    "#     widths = boxes[:, 2] - boxes[:, 0] + 1.0\n",
    "#     heights = boxes[:, 3] - boxes[:, 1] + 1.0\n",
    "#     ctr_x = boxes[:, 0] + 0.5 * widths\n",
    "#     ctr_y = boxes[:, 1] + 0.5 * heights\n",
    "\n",
    "#     dx = deltas[:, 0::4]\n",
    "#     dy = deltas[:, 1::4]\n",
    "#     dw = deltas[:, 2::4]\n",
    "#     dh = deltas[:, 3::4]\n",
    "\n",
    "#     pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n",
    "#     pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n",
    "#     pred_w = np.exp(dw) * widths[:, np.newaxis]\n",
    "#     pred_h = np.exp(dh) * heights[:, np.newaxis]\n",
    "\n",
    "#     pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)\n",
    "#     # x1\n",
    "#     pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n",
    "#     # y1\n",
    "#     pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n",
    "#     # x2\n",
    "#     pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n",
    "#     # y2\n",
    "#     pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n",
    "\n",
    "#     return pred_boxes\n",
    "\n",
    "\n",
    "def getLargest(a,b):\n",
    "    if (a>b):\n",
    "        return a\n",
    "    else :\n",
    "        return b\n",
    "\n",
    "def getSmallest(a,b):\n",
    "    if(a<b):\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "    \n",
    "    \n",
    "def clip_boxes(boxes, im_shape_col, im_shape_row):\n",
    "    \"\"\"\n",
    "    Clip boxes to image boundaries.\n",
    "    \"\"\"\n",
    "    for box in boxes:\n",
    "        box[0] = getLargest( getSmallest(box[0], im_shape_col) ,0)\n",
    "        box[1] = getLargest( getSmallest(box[1], im_shape_row) ,0)\n",
    "        box[2] = getLargest( getSmallest(box[2], im_shape_col) ,0)\n",
    "        box[3] = getLargest( getSmallest(box[3], im_shape_row) ,0)\n",
    "            \n",
    "    return boxes\n",
    "\n",
    "def filter_boxs_by_size(boxes, threshold=30):\n",
    "    \n",
    "    ws = boxes[:, 2] - boxes[:, 0] + 1\n",
    "    hs = boxes[:, 3] - boxes[:, 1] + 1\n",
    "    keep = np.where((ws >= threshold) & (hs >= threshold))[0]\n",
    "    return keep\n",
    "\n",
    "def nms(dets, thresh):\n",
    "    \"\"\"Pure Python NMS baseline.\"\"\"\n",
    "    x1 = dets[:, 0]\n",
    "    y1 = dets[:, 1]\n",
    "    x2 = dets[:, 2]\n",
    "    y2 = dets[:, 3]\n",
    "    scores = dets[:, 4]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep\n",
    "\n",
    "def rpn_proposal_layer(rpn_class, rpn_regr, img_input):\n",
    "    ''' \n",
    "    This function map anchor to the position, which return x,y,w,h with globel coor. \n",
    "    rather than anchor inner position.\n",
    "    img_input is use for get the imgSize ref.\n",
    "    '''\n",
    "    \n",
    "    print(\"rpn_class.shape\", rpn_class.shape)\n",
    "    print(\"rpn_regr.shape\", rpn_regr.shape)\n",
    "    print(\"img_input.shape\", img_input.shape)\n",
    "    \n",
    "    img_width = img_input.shape[2]\n",
    "    img_height = img_input.shape[1]\n",
    "    \n",
    "    feature_map_width = rpn_regr.shape[2]\n",
    "    feature_map_height = rpn_regr.shape[1]\n",
    "    \n",
    "    img_to_feature_map_ratio = img_width/feature_map_width\n",
    "    \n",
    "    nb_anchors = int(rpn_class.shape[3])\n",
    "    \n",
    "#     reshape the array by anchors \n",
    "    rpn_class_reshaped = np.reshape(rpn_class, (int(rpn_class.shape[1]), int(rpn_class.shape[2]),nb_anchors, 1))\n",
    "    rpn_regr_reshaped = np.reshape(rpn_regr, (int(rpn_regr.shape[1]), int(rpn_regr.shape[2]),nb_anchors, 4)) \n",
    "    \n",
    "    class_res, regr_res = mapAnchorToBoxs(rpn_class_reshaped, rpn_regr_reshaped, img_to_feature_map_ratio)\n",
    "    \n",
    "    # 2. clip predicted boxes to image\n",
    "    regr_res = clip_boxes(regr_res, int(rpn_class.shape[2]), int(rpn_class.shape[1]))\n",
    "    print(\"class_res.shape\",class_res.shape)\n",
    "    print(\"regr_res.shape\",regr_res.shape)\n",
    "    \n",
    "    # filter by size\n",
    "    id_keep = filter_boxs_by_size(regr_res, 30)\n",
    "    print(\"id_keep.shape\",id_keep.shape)\n",
    "    class_res_keep = class_res[id_keep]\n",
    "    regr_res_keep = regr_res[id_keep, :]\n",
    "    \n",
    "    # sort \n",
    "    # getTop N \n",
    "    pre_nms_topN = 6000\n",
    "    \n",
    "    order = class_res_keep.ravel().argsort()[::-1]\n",
    "    if pre_nms_topN > 0:\n",
    "        order = order[:pre_nms_topN]\n",
    "    proposals = regr_res_keep[order, :]\n",
    "    scores = class_res_keep[order]\n",
    "    \n",
    "    print(\"proposals.shape\",proposals.shape)    \n",
    "    print(\"scores.shape\",scores.shape)\n",
    "    \n",
    "    # 6. apply nms (e.g. threshold = 0.7)\n",
    "    # 7. take after_nms_topN (e.g. 300)\n",
    "    # 8. return the top proposals (-> RoIs top)\n",
    "    post_nms_topN = 300\n",
    "    nms_thresh = 0.6\n",
    "    keep = nms(np.hstack((proposals, scores)), nms_thresh)\n",
    "    if post_nms_topN > 0:\n",
    "        keep = keep[:post_nms_topN]\n",
    "    proposals = proposals[keep, :]\n",
    "    scores = scores[keep]\n",
    "    print(\"proposals.shape\",proposals.shape)    \n",
    "    print(\"scores.shape\",scores.shape)\n",
    "    \n",
    "    return proposals.astype(np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getBoxIds(poposal_res):\n",
    "    print(\"poposal_res.shape\",poposal_res.shape)\n",
    "    return np.zeros(len(poposal_res)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss\n",
    "\n",
    "-------------\n",
    "#### Training RPNs \n",
    "I fllow the method in paper, which random sample 256 anchors in image and make pos. neg anchor be 1:1 ratio. \n",
    "#### Method to assign pos. label to anchor\n",
    "1. the anchor with highest intersection-over-Union (IoU) overla[ with a geound truth box or\n",
    "2. an anchor that has an IoU overlap with higher than 0.7 withany ground truth box \n",
    "\n",
    "#### Method to assign neg. label to anchor \n",
    "1. IoU ratio is lower than 0.3 for all ground truth boxes\n",
    "\n",
    "if anchor neither pos. and neg. do not contribute to the training objective "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth_l1(self, sigma, bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights):\n",
    "    \"\"\"\n",
    "        ResultLoss = outside_weights * SmoothL1(inside_weights * (bbox_pred - bbox_targets))\n",
    "        SmoothL1(x) = 0.5 * (sigma * x)^2,    if |x| < 1 / sigma^2\n",
    "                      |x| - 0.5 / sigma^2,    otherwise\n",
    "    \"\"\"\n",
    "    sigma2 = sigma * sigma\n",
    "\n",
    "    x = tf.subtract(bbox_pred, bbox_targets)\n",
    "\n",
    "    smooth_l1_sign = tf.cast(tf.less(tf.abs(x), 1.0 / sigma2), tf.float32)\n",
    "    smooth_l1_option1 = tf.multiply(tf.multiply(x, x), 0.5 * sigma2)\n",
    "    smooth_l1_option2 = tf.subtract(tf.abs(x), 0.5 / sigma2)\n",
    "    smooth_l1_result = tf.add(tf.multiply(smooth_l1_option1, smooth_l1_sign),\n",
    "                              tf.multiply(smooth_l1_option2, tf.abs(tf.subtract(smooth_l1_sign, 1.0))))\n",
    "\n",
    "    return smooth_l1_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_rpn_regr = 1.0\n",
    "lambda_rcnn_regr = 1.0\n",
    "lambda_rpn_class = 1.0\n",
    "lambda_rcnn_class = 1.0\n",
    "\n",
    "esilon = 1e-4\n",
    "\n",
    "\n",
    "def rpn_regr_loss(y_pred, y_true_regr, y_true_cls):\n",
    "    x = y_pred - y_true_regr \n",
    "    x_abs = tf.abs(x)\n",
    "    x_bool = tf.cast(tf.less_equal(x_abs, 1.0), tf.float32)\n",
    "    \n",
    "    x_smooth_l1 = (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))\n",
    "    x_smooth_l1_pos_only = y_true_cls * x_smooth_l1\n",
    "    \n",
    "    return lambda_rpn_regr * x_smooth_l1_pos_only / tf.sum(epsilon + y_true_cls)\n",
    "    \n",
    "def rpn_class_loss(y_pred, y_true_cls):\n",
    "    x = y_true_cls * tf.binary_crossentropy(y_pred, y_true_cls)\n",
    "    return lambda_rpn_class * tf.sum(x) / tf.sum(epsilon + y_true_cls)\n",
    "\n",
    "# background is id = 0\n",
    "def rcnn_regr_loss(y_pred, y_true_regr, y_true_cls):\n",
    "    x = y_pred - y_true_regr\n",
    "    x_abs = tf.abs(x)\n",
    "    x_bool = tf.cast(tf.less_equal(x_abs, 1.0), tf.float32)\n",
    "    \n",
    "    x_smooth_l1 = (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))\n",
    "    x_smooth_l1_pos_only = y_true_cls * x_smooth_l1\n",
    "    \n",
    "    return lambda_rpn_regr * x_smooth_l1_pos_only / tf.sum(epsilon + y_true_cls)\n",
    "    \n",
    "\n",
    "def rcnn_cls_loss(y_pred, y_true):\n",
    "    softmax_cross_entropy = tf.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    return lambda_rcnn_class * tf.reduce_mean(softmax_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rpn_ground_truth(x_image, y_bbox_input):\n",
    "    '''\n",
    "        1. get the feature map size \n",
    "        2. generate two matrix ground truth, cls ground truth base on bbox \n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def rpn_y_bbox(rpn_ground_truth_res):\n",
    "    return rpn_ground_truth_res[0]\n",
    "\n",
    "def rpn_y_cls(rpn_ground_truth_res):\n",
    "    return rpn_ground_truth_res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore conv layers\n",
      "rpn_class.shape (1, 180, 320, 9)\n",
      "rpn_regr.shape (1, 180, 320, 36)\n",
      "img_input.shape (1, 720, 1280, 3)\n",
      "class_res.shape (518400, 1)\n",
      "regr_res.shape (518400, 4)\n",
      "id_keep.shape (211606,)\n",
      "proposals.shape (6000, 4)\n",
      "scores.shape (6000, 1)\n",
      "proposals.shape (85, 4)\n",
      "scores.shape (85, 1)\n",
      "poposal_res.shape (85, 4)\n"
     ]
    }
   ],
   "source": [
    "nb_anchors = len(anchor_box_scales) * len(anchor_box_ratio)\n",
    "\n",
    "img_input = tf.placeholder(tf.float32, [1, None, None, 3])\n",
    "\n",
    "# input the bbox location and the class id \n",
    "# in format x,y,w,h,classId\n",
    "y_bbox_input = tf.placeholder(tf.float32, [1, None, 5])\n",
    "\n",
    "# generate rpn ground truth \n",
    "rpn_y_res = tf.py_func(rpn_ground_truth, [img_input, y_bbox_input], tf.float32, name=\"rpn_y_res\")\n",
    "rpn_y_bbox_res = tf.py_func(rpn_y_bbox,rpn_y_res, tf.float32, name=\"rpn_y_bbox_res\")\n",
    "rpn_y_cls_res = tf.py_func(rpn_y_cls,rpn_y_res, tf.float32, name=\"rpn_y_cls_res\")\n",
    "\n",
    "# define the crop size of roi pooling \n",
    "crop_size = tf.constant([14,14])\n",
    "\n",
    "conv_layer,conv_end_points = vgg_16(img_input)\n",
    "conv_restore_names = [ item for item in conv_end_points] \n",
    "\n",
    "rpn_class, rpn_regr, rpn_end_points = rpn(conv_layer,nb_anchors)\n",
    "rpn_class_softmax = tf.nn.softmax(rpn_class)\n",
    "\n",
    "# poposal layer\n",
    "roi_proposal = tf.py_func(rpn_proposal_layer,[rpn_class_softmax, rpn_regr, img_input], tf.float32, name=\"roi_proposal\")\n",
    "\n",
    "rpn_loss_regr = rpn_regr_loss(rpn_regr, rpn_y_bbox_res)\n",
    "rpn_loss_cls = rpn_class_loss(rpn_class, rpn_y_cls_res)\n",
    "rpn_loss = rpn_loss_regr + rpn_loss_cls\n",
    "\n",
    "rpn_optimizer = tf.train.AdamOptimizer(0.005).minimize(rpn_loss)\n",
    "\n",
    "# pooling layer\n",
    "get_box_ids = tf.py_func(getBoxIds, [roi_proposal],  tf.int32 );\n",
    "\n",
    "roi_pooling = tf.image.crop_and_resize(conv_layer,roi_proposal, get_box_ids, crop_size)\n",
    "roi_pooling = tf.nn.max_pool(roi_pooling, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],  padding='SAME')\n",
    "\n",
    "flatten = slim.flatten(roi_pooling, scope=\"flatten\")\n",
    "fc6 = slim.fully_connected(flatten, 4096)\n",
    "fc7 = slim.fully_connected(fc6, 4096)\n",
    "\n",
    "cls_score = slim.fully_connected(fc7, 21)  # 20 class + bg \n",
    "cls_score = tf.nn.softmax(cls_score)\n",
    "\n",
    "bbox_pred = slim.fully_connected(fc7, 84) # each class have it's own bbox\n",
    "\n",
    "\n",
    "\n",
    "# rcnn_loss_regr = rcnn_regr_loss()\n",
    "# rcnn_loss_cls = rcnn_cls_loss()\n",
    "# rcnn_loss = rcnn_loss_regr + rcnn_loss_cls\n",
    "# rpn_optimizer = tf.train.AdamOptimizer(0.005).minimize(rcnn_loss)\n",
    "\n",
    "\n",
    "# restore weights\n",
    "variables_to_restore = slim.get_variables_to_restore(include=conv_restore_names)\n",
    "vgg_checkpoint_path = os.path.join(\"./\", 'vgg_16.ckpt')\n",
    "restorer = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    restorer.restore(sess, \"./vgg_16.ckpt\")\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    print(\"restore conv layers\")\n",
    "    \n",
    "    res, rpn_class_res, rpn_regr_res, roi_proposal_res, roi_pooling_res, bbox_pred_res, cls_score_res = sess.run([conv_layer, rpn_class, rpn_regr, roi_proposal, roi_pooling, bbox_pred, cls_score], feed_dict={img_input:TEST_FULL_IMG})\n",
    "#     plt.imshow(res[0], cmap='gray')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " res.shape (1, 180, 320, 512) \n",
      " rpn_class_res.shape (1, 180, 320, 9) \n",
      " rpn_regr_res.shape (1, 180, 320, 36) \n",
      " roi_proposal_res.shape (85, 4) \n",
      " roi_pooling_res.shape (85, 7, 7, 512) \n",
      " bbox_pred.shape (85, 84) cls_score.shape (85, 21)\n"
     ]
    }
   ],
   "source": [
    "print(\" res.shape\",res.shape, \"\\n rpn_class_res.shape\",rpn_class_res.shape, \"\\n rpn_regr_res.shape\", rpn_regr_res.shape, \"\\n roi_proposal_res.shape\", roi_proposal_res.shape, \"\\n roi_pooling_res.shape\", roi_pooling_res.shape, \"\\n bbox_pred.shape\",bbox_pred_res.shape, \"cls_score.shape\",cls_score_res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
