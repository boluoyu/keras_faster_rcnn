{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-Faster-RCNN-RPN\n",
    "\n",
    "----------------------\n",
    "I will implement RPN network in this notebook. \n",
    "\n",
    "\n",
    "#### Objective of this notebook \n",
    "1. Trian a rpn for object localization \n",
    "\n",
    "\n",
    "### Section list\n",
    "\n",
    "1. Define a generator that return (1, height, width, 3), (1, nb_boxes, 4), (1, nb_boxes, 1)\n",
    "2. Define the conv. layers \n",
    "3. Define the rpn layers \n",
    "4. Define the loss tensors\n",
    "5. build network \n",
    "6. Train the network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improt dependency  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib.framework.python.ops import arg_scope\n",
    "from tensorflow.contrib.layers.python.layers import layers as layers_lib\n",
    "from tensorflow.contrib.layers.python.layers import regularizers\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import variable_scope\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from math import floor,exp\n",
    "import pprint\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the global varables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 720, 1280, 3)\n",
      "180.0 320.0\n"
     ]
    }
   ],
   "source": [
    "anchor_box_scales = [128, 256, 512]\n",
    "anchor_box_ratio = [[1,1],[1,2],[2,1]]\n",
    "nb_anchors = len(anchor_box_scales) * len(anchor_box_ratio)\n",
    "\n",
    "dataSets = ['VOC2012']\n",
    "\n",
    "\n",
    "TEST_FULL_IMG = np.array([mpimg.imread(\"./test1.jpg\")])\n",
    "print(TEST_FULL_IMG.shape)\n",
    "print(TEST_FULL_IMG.shape[1]/4, TEST_FULL_IMG.shape[2]/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define generator\n",
    "\n",
    "We use VOC datasets in this implmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_voc_data(input_path):\n",
    "    all_imgs = []\n",
    "\n",
    "    classes_count = {}\n",
    "\n",
    "    class_mappingNameToId = {}\n",
    "    class_mappingIdToName = {}\n",
    "\n",
    "    visualise = False\n",
    "\n",
    "    data_paths = [os.path.join(input_path, s) for s in dataSets]\n",
    "\n",
    "    print ('Parsing annotation files')\n",
    "\n",
    "    for data_path in data_paths:\n",
    "\n",
    "        annot_path = os.path.join(data_path, 'Annotations')\n",
    "        imgs_path = os.path.join(data_path, 'JPEGImages')\n",
    "        imgsets_path_trainval = os.path.join(data_path, 'ImageSets',\n",
    "                'Main', 'trainval.txt')\n",
    "\n",
    "# ........imgsets_path_test = os.path.join(data_path, 'ImageSets','Main','test.txt')\n",
    "\n",
    "        trainval_files = []\n",
    "        test_files = []\n",
    "        try:\n",
    "            with open(imgsets_path_trainval) as f:\n",
    "                for line in f:\n",
    "                    trainval_files.append(line.strip() + '.jpg')\n",
    "        except Exception as e:\n",
    "\n",
    "# ............with open(imgsets_path_test) as f:\n",
    "# ................for line in f:\n",
    "# ....................test_files.append(line.strip() + '.jpg')\n",
    "\n",
    "            print (e)\n",
    "\n",
    "        annots = [os.path.join(annot_path, s) for s in\n",
    "                  os.listdir(annot_path)]\n",
    "        idx = 0\n",
    "        for annot in annots:\n",
    "            try:\n",
    "                idx += 1\n",
    "\n",
    "                et = ET.parse(annot)\n",
    "                element = et.getroot()\n",
    "\n",
    "                element_objs = element.findall('object')\n",
    "                element_filename = element.find('filename').text\n",
    "                element_width = int(element.find('size').find('width'\n",
    "                                    ).text)\n",
    "                element_height = int(element.find('size').find('height'\n",
    "                        ).text)\n",
    "\n",
    "                if len(element_objs) > 0:\n",
    "                    annotation_data = {\n",
    "                        'filepath': os.path.join(imgs_path,\n",
    "                                element_filename),\n",
    "                        'width': element_width,\n",
    "                        'height': element_height,\n",
    "                        'bboxes': [],\n",
    "                        }\n",
    "\n",
    "                    if element_filename in trainval_files:\n",
    "                        annotation_data['imageset'] = 'trainval'\n",
    "                    elif element_filename in test_files:\n",
    "                        annotation_data['imageset'] = 'test'\n",
    "                    else:\n",
    "                        annotation_data['imageset'] = 'trainval'\n",
    "\n",
    "                for element_obj in element_objs:\n",
    "                    class_name = element_obj.find('name').text\n",
    "                    if class_name not in classes_count:\n",
    "                        classes_count[class_name] = 1\n",
    "                    else:\n",
    "                        classes_count[class_name] += 1\n",
    "\n",
    "                    if class_name not in class_mappingNameToId:\n",
    "                        class_mappingNameToId[class_name] = \\\n",
    "                            len(class_mappingNameToId)\n",
    "                        class_mappingIdToName[len(class_mappingNameToId)\n",
    "                                - 1] = class_name\n",
    "\n",
    "                    obj_bbox = element_obj.find('bndbox')\n",
    "                    x1 = int(round(float(obj_bbox.find('xmin').text)))\n",
    "                    y1 = int(round(float(obj_bbox.find('ymin').text)))\n",
    "                    x2 = int(round(float(obj_bbox.find('xmax').text)))\n",
    "                    y2 = int(round(float(obj_bbox.find('ymax').text)))\n",
    "                    w = x2 - x1\n",
    "                    h = y2 - y1\n",
    "                    x = int(round(x1 + w / 2))\n",
    "                    y = int(round(y1 + h / 2))\n",
    "                    difficulty = int(element_obj.find('difficult'\n",
    "                            ).text) == 1\n",
    "                    annotation_data['bboxes'].append({\n",
    "                        'class': class_name,\n",
    "                        'x1': x1,\n",
    "                        'x2': x2,\n",
    "                        'y1': y1,\n",
    "                        'y2': y2,\n",
    "                        'difficult': difficulty,\n",
    "                        'x': x,\n",
    "                        'y': y,\n",
    "                        'w': w,\n",
    "                        'h': h,\n",
    "                        })\n",
    "                all_imgs.append(annotation_data)\n",
    "\n",
    "                if visualise:\n",
    "                    img = cv2.imread(annotation_data['filepath'])\n",
    "                    for bbox in annotation_data['bboxes']:\n",
    "                        cv2.rectangle(img, (bbox['x1'], bbox['y1']),\n",
    "                                (bbox['x2'], bbox['y2']), (0, 0, 255))\n",
    "                    cv2.imshow('img', img)\n",
    "                    cv2.waitKey(0)\n",
    "            except Exception as  e:\n",
    "\n",
    "                print (e)\n",
    "                continue\n",
    "\n",
    "        # make if no bg in the className make bg class\n",
    "\n",
    "        if 'bg' not in classes_count:\n",
    "            print ('bg not in class')\n",
    "            \n",
    "            class0Name = class_mappingIdToName[0]\n",
    "            class0NewId = len(class_mappingNameToId)\n",
    "            \n",
    "            \n",
    "            classes_count['bg'] = 0\n",
    "            class_mappingNameToId['bg'] = 0 \n",
    "            class_mappingIdToName[0] = 'bg'\n",
    "            \n",
    "            class_mappingNameToId[class0Name] = class0NewId\n",
    "            class_mappingIdToName[len(class_mappingNameToId)-1] = class0Name\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print ('bg in class')\n",
    "            # if there are a bg class make it first index\n",
    "            bgOldId = class_mappingNameToId['bg']\n",
    "            #switch id bg to 0id class\n",
    "            if bgOldId != 0 :\n",
    "                class0Name = class_mappingIdToName[0]\n",
    "                class0NewId = bgOldId\n",
    "                \n",
    "                class_mappingIdToName[0] = 'bg'\n",
    "                class_mappingNameToId['bg'] = 0\n",
    "                \n",
    "                class_mappingIdToName[class0NewId] = class0Name\n",
    "                class_mappingNameToId[class0Name] = class0NewId\n",
    "            \n",
    "    return (all_imgs, classes_count, class_mappingNameToId, class_mappingIdToName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Data_source:\n",
    "    def __init__(self):\n",
    "        all_imgs, classes_count, class_mappingNameToId, class_mappingIdToName = get_voc_data(\"./data\")\n",
    "        self.all_imgs = all_imgs\n",
    "        self.classes_count = classes_count\n",
    "        self.class_mappingNameToId = class_mappingNameToId\n",
    "        self.class_mappingIdToName = class_mappingIdToName\n",
    "        \n",
    "    def get_classId(self, class_name):\n",
    "        return self.class_mappingNameToId[class_name]\n",
    "    \n",
    "    def get_generator(self):\n",
    "        '''\n",
    "            generator use in training\n",
    "            imgs \n",
    "            x,y,w,h\n",
    "            x1,y1,x2,y2\n",
    "            class\n",
    "            \n",
    "            return (1, height, width, 3), (1, nb_boxes, 4), (1, nb_boxes, 4), (1, nb_boxes, 1)\n",
    "        '''\n",
    "        records = sklearn.utils.shuffle(self.all_imgs)\n",
    "        for record in records:\n",
    "            img = mpimg.imread(record[\"filepath\"])\n",
    "            \n",
    "            box_xywh = []\n",
    "            box_xyxy = []\n",
    "            box_class = []\n",
    "            \n",
    "            for box in record[\"bboxes\"]:\n",
    "                box_xywh.append([box[\"x\"], box[\"y\"], box[\"w\"], box[\"h\"]])\n",
    "                box_xyxy.append([box[\"x1\"], box[\"y1\"], box[\"x2\"], box[\"y2\"]])\n",
    "                name = self.get_classId(box[\"class\"])\n",
    "                box_class.append([name])\n",
    "        \n",
    "            yield np.array([img]), np.array([box_xywh]), np.array([box_xyxy]), np.array([box_class])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotation files\n",
      "bg not in class\n"
     ]
    }
   ],
   "source": [
    "datas = Data_source()\n",
    "generator = datas.get_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "img, xtwh,xxyy, cls = next(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the conv. layers\n",
    "\n",
    "---------\n",
    "\n",
    "I will define vgg16 conv layers. \n",
    "\n",
    "Vgg 16 used in this project, it use the pretrain network from imageNet. The network defined until conv5.\n",
    "\n",
    "The vgg16 function accept iamge input and otput the nets tensor and the enpoints.\n",
    "\n",
    "We use conv5 as our feature map layer, which will port to rpn and rcnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the vgg16 layers\n",
    "def vgg_16(inputs,  scope='vgg_16'):\n",
    "    with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n",
    "        end_points_collection = sc.name + '_end_points'\n",
    "\n",
    "        # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d], outputs_collections=end_points_collection):\n",
    "            net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "#             net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "#             net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "\n",
    "            # Convert end_points_collection into a end_point dict.\n",
    "            end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "        \n",
    "    return net, end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the rpn layers\n",
    "\n",
    "\n",
    "we will define the RPN network. \n",
    "\n",
    "It have few steps. \n",
    "1. Define the conv layers of rpn.\n",
    "2. Mapping rpn to bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def rpn_cls_shape_func(in_list):\n",
    "    cls = in_list\n",
    "    return np.array([cls.shape[0], cls.shape[1],cls.shape[2],cls.shape[3],1]).astype(np.int32)\n",
    "\n",
    "def rpn_regr_shape_func(in_list):\n",
    "    regr = in_list\n",
    "    return np.array([regr.shape[0], regr.shape[1],regr.shape[2],regr.shape[3]/4,4]).astype(np.int32)\n",
    "\n",
    "def rpn(net, num_anchors=9, scope=\"rpn\"):\n",
    "    with tf.variable_scope(scope, 'rpn', [net]) as sc:\n",
    "        end_points_collection = sc.name + '_end_points'\n",
    "        \n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d], \n",
    "                            outputs_collections=end_points_collection, \n",
    "                            activation_fn=tf.nn.relu,\n",
    "                            weights_initializer=tf.truncated_normal_initializer(0.0, 0.01)):\n",
    "            \n",
    "            net = slim.conv2d(net, 512, [3, 1], scope='rpn_conv_3x3', padding='SAME')\n",
    "            \n",
    "            rpn_class = slim.conv2d(net, num_anchors, [1, 1], scope='rpn_class')\n",
    "            rpn_cls_shape = tf.py_func(rpn_cls_shape_func, [rpn_class], tf.int32, name=\"rpn_cls_shape\")\n",
    "            rpn_class = tf.reshape(rpn_class, rpn_cls_shape)\n",
    "            \n",
    "            rpn_regr = slim.conv2d(net, num_anchors*4, [1, 1], scope='rpn_regr')  \n",
    "            rpn_regr_shape = tf.py_func(rpn_regr_shape_func, [rpn_regr], tf.int32, name=\"rpn_regr_shape\")\n",
    "            rpn_regr = tf.reshape(rpn_regr, rpn_regr_shape)\n",
    "\n",
    "            # Convert end_points_collection into a end_point dict.\n",
    "            end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "        \n",
    "    return rpn_class, rpn_regr, end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_generate_anchors( anchor_box_scales = [128, 256, 512],   anchor_box_ratio = [[1,1],[1,2],[2,1]] ):\n",
    "    def generate_anchors(featureMap, img):\n",
    "        '''\n",
    "            Not batch is return, which can save memory in run time \n",
    "        '''\n",
    "        stepSize = int(img.shape[1]/featureMap.shape[1])\n",
    "        imgWidth = img.shape[2]\n",
    "        imgheight = img.shape[1]\n",
    "        \n",
    "        anchors = []\n",
    "        for scale in anchor_box_scales:\n",
    "            for ratio in anchor_box_ratio:\n",
    "                anchors.append([ratio[0]*scale,ratio[1]*scale])\n",
    "        anchors = np.array(anchors)\n",
    "\n",
    "        bbox = np.zeros((featureMap.shape[1], featureMap.shape[2], len(anchors), 4))\n",
    "\n",
    "#         base on the feature map that input to this function \n",
    "        x = range(int(stepSize/2), featureMap.shape[2]*stepSize, stepSize)\n",
    "        y = range(int(stepSize/2), featureMap.shape[1]*stepSize, stepSize)\n",
    "\n",
    "        xv, yv= np.meshgrid(x, y)\n",
    "\n",
    "        for anchorIdx, width, height in zip(range(len(anchors)), anchors[:,0], anchors[:,1]):\n",
    "            bbox[:,:,anchorIdx,0] = xv\n",
    "            bbox[:,:,anchorIdx,1] = yv\n",
    "            bbox[:,:,anchorIdx,2].fill(width)\n",
    "            bbox[:,:,anchorIdx,3].fill(height)\n",
    "        bbox = bbox.astype(np.int32)\n",
    "\n",
    "        return bbox\n",
    "    return generate_anchors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def union(au, bu):\n",
    "    x = min(au[0], bu[0])\n",
    "    y = min(au[1], bu[1])\n",
    "    w = max(au[2], bu[2]) - x\n",
    "    h = max(au[3], bu[3]) - y\n",
    "    return x, y, w, h\n",
    "\n",
    "def intersection(ai, bi):\n",
    "    x = max(ai[0], bi[0])\n",
    "    y = max(ai[1], bi[1])\n",
    "    w = min(ai[2], bi[2]) - x\n",
    "    h = min(ai[3], bi[3]) - y\n",
    "    if w < 0 or h < 0:\n",
    "        return 0, 0, 0, 0\n",
    "    return x, y, w, h\n",
    "\n",
    "def iou(a_, b_):\n",
    "#   a, b should be x,y,w,h gormat\n",
    "    # a and b should be (x1,y1,x2,y2)\n",
    "    x1 = a_[0] - a_[2]/2\n",
    "    y1 = a_[1] - a_[3]/2\n",
    "    a = [x1, y1, x1+a_[2], y1 + a_[2]]\n",
    "    \n",
    "    x1 = b_[0] - b_[2]/2\n",
    "    y1 = b_[1] - b_[3]/2\n",
    "    b = [x1, y1, x1+b_[2], y1 + b_[2]]\n",
    "    \n",
    "    if a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:\n",
    "        return 0.0\n",
    "\n",
    "    i = intersection(a, b)\n",
    "    u = union(a, b)\n",
    "\n",
    "    area_i = i[2] * i[3]\n",
    "    area_u = u[2] * u[3]\n",
    "    return float(area_i) / float(area_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "def testIoU():\n",
    "    box_a = [50,50,50,50]\n",
    "    box_b = [50,55,50,70]\n",
    "    v = iou(box_a, box_b)\n",
    "    print(v)\n",
    "    assert( v> 0.7)\n",
    "testIoU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cal_parameterizations_bbox(ax, ay, aw, ah, bx, by, bw, bh):\n",
    "    return [(ax-bx)/bw, (ay-by)/bh, exp(aw/bw), exp(ah/bh)]\n",
    "\n",
    "\n",
    "def create_rpn_ground_truth(anchors, bboxes, bboxes_cls):\n",
    "    '''\n",
    "        Since it is training one img is accepted \n",
    "        input shape = (1,None,4) in x,y,w,h\n",
    "        return anchorbox with t_x, t_y, t_w, t_h and anchorBoxLabel with -1 neg, 1 is pos, 0 is nth\n",
    "    '''\n",
    "    \n",
    "    regr_y = np.zeros(anchors.shape)\n",
    "    cls_shape = np.array(anchors.shape)\n",
    "    cls_shape[-1] = 1\n",
    "    cls_y = np.zeros(cls_shape)\n",
    "    \n",
    "    print(\"cls_shape\",cls_shape)\n",
    "    print(\"anchors.shape\",anchors.shape)\n",
    "    print(\"bboxes.shape\",bboxes.shape)\n",
    "    print(\"bboxes_cls.shape\",bboxes_cls.shape)\n",
    "    \n",
    "    highest_iou = (0,[])\n",
    "    \n",
    "    for box, box_class in zip(bboxes[0],bboxes_cls[0]):\n",
    "        # skip bg class\n",
    "        if box_class == 0:\n",
    "            continue\n",
    "        pos_list = []\n",
    "        neg_list = []\n",
    "        for y in range(anchors.shape[0]):\n",
    "            for x in range(anchors.shape[1]):\n",
    "                for a in range(anchors.shape[2]):\n",
    "                    anchor = anchors[y,x,a]\n",
    "                    regr   = regr_y[y,x,a]\n",
    "                    cls    = cls_y[y,x,a]\n",
    "                    \n",
    "                    # cal the iou \n",
    "                    iou_v = iou(anchor, box)\n",
    "                    \n",
    "                    # if iou <= 0.3\n",
    "#                        add to neg list\n",
    "                    if iou_v <= 0.3:\n",
    "#                         if(abs(anchor[0]-box[0]) < 50 and abs(anchor[1]-box[1]) < 50):\n",
    "#                             print(\"anchor, box, iou_v\",anchor, box, iou_v)\n",
    "                        neg_list.append([y,x,a])\n",
    "                    # if iou >= 0.7\n",
    "#                        add to pos list\n",
    "                    elif iou_v >= 0.7:\n",
    "                        pos_list.append([y,x,a])\n",
    "                    else:\n",
    "                        if iou_v > highest_iou[0]:\n",
    "                            highest_iou = (iou_v,[y,x,a])\n",
    "                    \n",
    "        print(\"highest_iou\",highest_iou)\n",
    "        print(\"len(pos_list)\",len(pos_list))\n",
    "        print(\"len(neg_list)\",len(neg_list))\n",
    "        print(\"box\",box)\n",
    "        \n",
    "        \n",
    "        if len(pos_list) == 0:\n",
    "            pos_list.append(highest_iou[1])\n",
    "            \n",
    "            \n",
    "        pos_list = np.array(pos_list)\n",
    "        neg_list = np.array(neg_list)\n",
    "        \n",
    "        \n",
    "#       filter max 256 or balance the smapling in ratio 1:1       \n",
    "        smaple_size = min(len(pos_list), len(neg_list), 256)\n",
    "    \n",
    "        print(\"min(len(pos_list), len(neg_list), 256)\", len(pos_list), len(neg_list), 256)\n",
    "        idx_pos = np.random.randint(len(pos_list), size=smaple_size)\n",
    "        idx_neg = np.random.randint(len(neg_list), size=smaple_size)\n",
    "            \n",
    "        print(\"len(idx_pos)\",len(idx_pos))\n",
    "        print(\"len(idx_neg)\",len(idx_neg))\n",
    "\n",
    "#       label the cls and cal. the parameterizations \n",
    "        selected_pos = pos_list[idx_pos]\n",
    "        for r_pos in selected_pos:\n",
    "            y = r_pos[0]\n",
    "            x = r_pos[1]\n",
    "            a = r_pos[2]\n",
    "            anchor = anchors[y,x,a]\n",
    "            regr_y[y,x,a] = cal_parameterizations_bbox(box[0], box[1], box[2], box[3], anchor[0], anchor[1], anchor[2], anchor[3])\n",
    "            cls_y[y,x,a] = 1\n",
    "        \n",
    "        selected_neg = neg_list[idx_neg]\n",
    "        for r_neg in selected_neg:\n",
    "            y = r_neg[0]\n",
    "            x = r_neg[1]\n",
    "            a = r_neg[2]\n",
    "            anchor = anchors[y,x,a]\n",
    "            regr_y[y,x,a] = cal_parameterizations_bbox(box[0], box[1], box[2], box[3], anchor[0], anchor[1], anchor[2], anchor[3])\n",
    "            cls_y[y,x,a] = -1\n",
    "        \n",
    "#   select highest iou if none in the pos list            \n",
    "#     print(cls_y)            \n",
    "    \n",
    "    return regr_y.astype(np.float32), cls_y.astype(np.float32)\n",
    " \n",
    "def cls_ground_truth(res):\n",
    "    return res[1]\n",
    "\n",
    "def regr_ground_truth(res):\n",
    "    return res[0]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_rpn_regr = 1.0\n",
    "lambda_rpn_class = 1.0\n",
    "\n",
    "\n",
    "esilon = 1e-4\n",
    "\n",
    "\n",
    "def smooth_l1(bbox_pred, bbox_targets, name=\"\"):\n",
    "    \"\"\"\n",
    "        ResultLoss = outside_weights * SmoothL1(inside_weights * (bbox_pred - bbox_targets))\n",
    "        SmoothL1(x) = 0.5 * ( x)^2,    if |x| < 1\n",
    "                      |x| - 0.5 ,    otherwise\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"smooth_l1_\"+name):\n",
    "        x = tf.subtract(bbox_pred, bbox_targets)\n",
    "\n",
    "        smooth_l1_sign = tf.cast(tf.less(tf.abs(x), 1.0 ), tf.float32)\n",
    "        smooth_l1_option1 = tf.multiply(tf.multiply(x, x), 0.5 )\n",
    "        smooth_l1_option2 = tf.subtract(tf.abs(x), 0.5 )\n",
    "        smooth_l1_result = tf.add(tf.multiply(smooth_l1_option1, smooth_l1_sign),\n",
    "                                  tf.multiply(smooth_l1_option2, tf.abs(tf.subtract(smooth_l1_sign, 1.0))))\n",
    "\n",
    "    return smooth_l1_result\n",
    "\n",
    "epsilon_tf = tf.constant(esilon, dtype=tf.float32, name=\"epsilon_tf\")\n",
    "\n",
    "def rpn_regr_loss(y_pred, y_true_regr, y_true_cls):\n",
    "    x = y_pred - y_true_regr \n",
    "    x_abs = tf.abs(x)\n",
    "    x_bool = tf.cast(tf.less_equal(x_abs, 1.0), tf.float32)\n",
    "    \n",
    "    x_smooth_l1 = (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))\n",
    "    x_smooth_l1_pos_only = y_true_cls * x_smooth_l1\n",
    "    \n",
    "    return lambda_rpn_regr * x_smooth_l1_pos_only / tf.reduce_sum(epsilon_tf + y_true_cls)\n",
    "    \n",
    "def rpn_class_loss(y_pred, y_true_cls):\n",
    "    x = y_true_cls * tf.binary_crossentropy(y_pred, y_true_cls)\n",
    "    return lambda_rpn_class * tf.sum(x) / tf.reduce_sum(epsilon_tf + y_true_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epsilon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-0cc7dcc436ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mrpn_loss_regr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpn_regr_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpn_regr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpn_regr_ground_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpn_cls_ground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mrpn_loss_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpn_class_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpn_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpn_cls_ground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mrpn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpn_loss_regr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrpn_loss_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-6a34eb9c34b7>\u001b[0m in \u001b[0;36mrpn_regr_loss\u001b[0;34m(y_pred, y_true_regr, y_true_cls)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mx_smooth_l1_pos_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true_cls\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_smooth_l1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlambda_rpn_regr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_smooth_l1_pos_only\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_true_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrpn_class_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epsilon' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "img_input = tf.placeholder(tf.float32, [1, None, None, 3])\n",
    "\n",
    "# input the bbox location and the class id \n",
    "# in format x,y,w,h,classId\n",
    "y_bbox_regr = tf.placeholder(tf.float32, [1, None, 4])\n",
    "\n",
    "y_bbox_cls = tf.placeholder(tf.float32, [1, None, 1])\n",
    "\n",
    "conv_layer,conv_end_points = vgg_16(img_input)\n",
    "conv_restore_names = [ item for item in conv_end_points] \n",
    "\n",
    "rpn_class, rpn_regr, rpn_end_points = rpn(conv_layer,nb_anchors)\n",
    "rpn_class_softmax = tf.nn.softmax(rpn_class)\n",
    "\n",
    "generate_anchors = tf.py_func(create_generate_anchors(), [rpn_regr, img_input], tf.int32, name=\"generate_anchors\")\n",
    "\n",
    "# generate grounth rpn regr. truth \n",
    "rpn_ground_truth = tf.py_func(create_rpn_ground_truth, [generate_anchors, y_bbox_regr, y_bbox_cls], [tf.float32, tf.int32], name=\"rpn_ground_truth\")\n",
    "\n",
    "\n",
    "rpn_cls_ground_truth  = tf.py_func(cls_ground_truth,  rpn_ground_truth, tf.float32, name=\"rpn_cls_ground_truth\")\n",
    "rpn_regr_ground_truth = tf.py_func(regr_ground_truth, rpn_ground_truth, [tf.float32], name=\"rpn_regr_ground_truth\")\n",
    "\n",
    "\n",
    "rpn_loss_regr = rpn_regr_loss(rpn_regr, rpn_regr_ground_truth, rpn_cls_ground_truth)\n",
    "rpn_loss_cls = rpn_class_loss(rpn_class, rpn_cls_ground_truth)\n",
    "rpn_loss = rpn_loss_regr + rpn_loss_cls\n",
    "\n",
    "rpn_optimizer = tf.train.AdamOptimizer(0.005).minimize(rpn_loss)\n",
    "\n",
    "\n",
    "# restore weights\n",
    "variables_to_restore = slim.get_variables_to_restore(include=conv_restore_names)\n",
    "vgg_checkpoint_path = os.path.join(\"./\", 'vgg_16.ckpt')\n",
    "restorer = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    restorer.restore(sess, \"./vgg_16.ckpt\")\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    print(\"restore conv layers\")\n",
    "    \n",
    "#   get data from generator \n",
    "    img, xywh, xxyy, cls = next(generator)\n",
    "    \n",
    "    target_tensor = [conv_layer, rpn_class_softmax, rpn_regr,generate_anchors, rpn_ground_truth]\n",
    "    feed_dict = {\n",
    "        img_input:img,\n",
    "        y_bbox_regr: xywh,\n",
    "        y_bbox_cls: cls\n",
    "    }\n",
    "    \n",
    "    res, rpn_class_res, rpn_regr_res, anchors, regr_y = sess.run(target_tensor, feed_dict=feed_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 117, 125, 9, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_class_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rpn_regr_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
